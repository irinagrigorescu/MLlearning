{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and Running with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igr18/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "### SETUP\n",
    "\n",
    "# For plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "%matplotlib inline\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Scikit learn \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# For scaling the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a computation graph\n",
    "\n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the graph with 2 variables and \n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "# the graph itself\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the graph (1)\n",
    "\n",
    "# Open a TF session\n",
    "sess = tf.Session()\n",
    "# Initialise variables in the session\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "# Evaluate f in the session\n",
    "result = sess.run(f)\n",
    "# Print result\n",
    "print(result)\n",
    "# Close session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the graph in an easier way (2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow program is typically split into two parts: the first part builds a computation graph (this is called the construction phase), and the second part runs it (this is the execution phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "### Managing graphs\n",
    "\n",
    "# Reset graph so it does not end up containing duplicate nodes\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create variable\n",
    "x1 = tf.Variable(1)\n",
    "# Will be in default graph\n",
    "print(x1.graph is tf.get_default_graph())\n",
    "\n",
    "# Create new graph \n",
    "graph = tf.Graph()\n",
    "# and temporarily make it the default graph inside a block\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    # Will print True because you are inside the with block\n",
    "    print(x2.graph is tf.get_default_graph())\n",
    "# Will print True as it is of graph\n",
    "print(x2.graph is graph)\n",
    "# Will print False as it is not the default graph\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n",
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "### Lifecycle of a Node Value\n",
    "\n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create constant w\n",
    "w = tf.constant(3)\n",
    "x = w+2\n",
    "y = x+5\n",
    "z = x*3\n",
    "\n",
    "# Runs them separately, meaning that it evaluates w and x twice\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())\n",
    "    \n",
    "# Runs them efficiently in one graph run\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF results: \n",
      " [[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]] \n",
      "\n",
      "NumPy results: \n",
      " [[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]] \n",
      "\n",
      "SciKit learn: \n",
      " [[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Fetch the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "# Get the shape of the dataset\n",
    "m, n = housing.data.shape\n",
    "\n",
    "# Add an extra bias input feature x_0 = 1 \n",
    "# to all training instances\n",
    "# housing has 8 features: MedInc, HouseAge, AveRooms, etc\n",
    "# after adding it has 9 features\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "# Create 2 TensorFlow constant nodes, X and y\n",
    "# to hold the data and the targets\n",
    "# X = 20640 x 9\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "# y = 20640 x 1\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# # Define X^T\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "# # Define theta = (X^T X)^-1 X^T y\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "\n",
    "# Create the session and evaluate theta\n",
    "with tf.Session() as sess: \n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "print(\"TF results: \\n\", theta_value, \"\\n\")\n",
    "\n",
    "## Compare with pure numpy\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(\"NumPy results: \\n\", theta_numpy, \"\\n\")\n",
    "\n",
    "## Compare with Scikit learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(\"SciKit learn: \\n\", np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "### Scaling the features first\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_housing_data]\n",
    "\n",
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  11.453259\n",
      "Epoch  100  MSE =  0.7859895\n",
      "Epoch  200  MSE =  0.60901916\n",
      "Epoch  300  MSE =  0.5845958\n",
      "Epoch  400  MSE =  0.56923044\n",
      "Epoch  500  MSE =  0.557992\n",
      "Epoch  600  MSE =  0.54967743\n",
      "Epoch  700  MSE =  0.543502\n",
      "Epoch  800  MSE =  0.53889894\n",
      "Epoch  900  MSE =  0.53545487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8592519 ],\n",
       "       [ 0.15122917],\n",
       "       [-0.27115855],\n",
       "       [ 0.28820804],\n",
       "       [ 0.00692668],\n",
       "       [-0.04270531],\n",
       "       [-0.6353016 ],\n",
       "       [-0.6069699 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### # GD \n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define variables\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create two TF constant nodes\n",
    "# to hold the data and the targets\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# Create 2 TF variables to store theta and the predictions\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Create the error variables\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate gradients\n",
    "# 1. Manually\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "# 2. Using TF's autodiff \n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "# Create a node that assigns a new value to a variable.\n",
    "# In this case it implements: theta^(k+1) = theta^(k) - eta grad\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# Initialise global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", epoch, \" MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  9.790122\n",
      "Epoch  100  MSE =  0.8067699\n",
      "Epoch  200  MSE =  0.62906545\n",
      "Epoch  300  MSE =  0.60218865\n",
      "Epoch  400  MSE =  0.5839549\n",
      "Epoch  500  MSE =  0.5702584\n",
      "Epoch  600  MSE =  0.55989265\n",
      "Epoch  700  MSE =  0.5520095\n",
      "Epoch  800  MSE =  0.54598475\n",
      "Epoch  900  MSE =  0.5413569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8950173 ],\n",
       "       [ 0.15788078],\n",
       "       [-0.33883458],\n",
       "       [ 0.344216  ],\n",
       "       [ 0.00888295],\n",
       "       [-0.04403077],\n",
       "       [-0.5532375 ],\n",
       "       [-0.5290735 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### # GD with optimiser \n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define variables\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create two TF constant nodes\n",
    "# to hold the data and the targets\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# Create 2 TF variables to store theta and the predictions\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Create the error variables\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate gradients with optimiser directly\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Initialise global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", epoch, \" MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5377837375479811"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yreal = housing.target.reshape(-1, 1)\n",
    "\n",
    "Ypred = np.transpose(np.matmul(np.transpose(best_theta), np.transpose(scaled_housing_data_plus_bias)))\n",
    "np.mean((Ypred - Yreal)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm\n",
    "\n",
    "To do mini-batch GD:\n",
    "> We need a way to replace X and y at every iteration with the next mini-batch. \n",
    "The simplest way to do this is to use ```placeholder``` nodes. \n",
    "These nodes are special because they don’t actually perform any computation, they just output the data you tell them to output at runtime.\n",
    "They are typically used to pass the training data to TensorFlow\n",
    "during training. If you don’t specify a value at runtime for a placeholder, you get an\n",
    "exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 20640)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholder nodes are used as input nodes \n",
    "# (just to output data, not evaluate it)\n",
    "A = tf.placeholder(tf.float32, shape = (None,3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]]})\n",
    "    \n",
    "print(B_val_1)    \n",
    "print(B_val_2) \n",
    "\n",
    "n,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0702524 ],\n",
       "       [ 0.85913193],\n",
       "       [ 0.12432421],\n",
       "       [-0.29847068],\n",
       "       [ 0.3707138 ],\n",
       "       [ 0.00520591],\n",
       "       [-0.0130143 ],\n",
       "       [-0.82407236],\n",
       "       [-0.79245   ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### # Mini-batch GD with placeholder nodes for the mini-batches\n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define variables\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# Create two TF placeholder nodes\n",
    "# to hold the data and the targets\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None,   1), name=\"y\")\n",
    "\n",
    "# Create 2 TF variables to store theta and the predictions\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Create the error variables\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate gradients with optimiser directly\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Initialise global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Function to fetch the mini-batch data\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)    # not shown in the book\n",
    "    indices = np.random.randint(m, size = batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]   # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]   # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "# Run the graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialise\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    # Get best theta\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph and Training Curves with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0552726 ],\n",
       "       [ 0.82219887],\n",
       "       [ 0.11761035],\n",
       "       [-0.25219893],\n",
       "       [ 0.32396972],\n",
       "       [ 0.01295227],\n",
       "       [-0.04279889],\n",
       "       [-0.9030713 ],\n",
       "       [-0.87828976]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### # Using TensorBoard\n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Add a time stamp to the log files, otherwise TB merges stats\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "# Define variables\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create two TF placeholder nodes\n",
    "# to hold the data and the targets\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Create 2 TF variables to store theta and the predictions\n",
    "theta  = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Create the error variables\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate gradients with optimiser directly\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Initialise global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "### Store stats:\n",
    "# Creates a node in the graph \n",
    "# that will evaluate the MSE value and write it\n",
    "# to a TensorBoard-compatible binary log string called a 'summary'\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "# Creates a FileWriter that you will use to write \n",
    "# summaries to logfiles in the log directory\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Function to fetch the mini-batch data\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)    # not shown in the book\n",
    "    indices = np.random.randint(m, size = batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]   # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]   # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "# Run the graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialise\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            ### Store stats about the MSE every 10 mini-batches\n",
    "            if batch_index % 10 == 0:\n",
    "                # Fetch the summary\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            # Run for next\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    # Get best theta\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "# Close the writer\n",
    "file_writer.close()\n",
    "\n",
    "best_theta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names Scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.0552726 ]\n",
      " [ 0.82219887]\n",
      " [ 0.11761035]\n",
      " [-0.25219893]\n",
      " [ 0.32396972]\n",
      " [ 0.01295227]\n",
      " [-0.04279889]\n",
      " [-0.9030713 ]\n",
      " [-0.87828976]]\n"
     ]
    }
   ],
   "source": [
    "### # Using name scopes to group related nodes\n",
    "# Reset graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Add a time stamp to the log files, otherwise TB merges stats\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "# Define variables\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create two TF placeholder nodes\n",
    "# to hold the data and the targets\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Create 2 TF variables to store theta and the predictions\n",
    "theta  = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "### Create the name scope here\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    # Create the error variables\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate gradients with optimiser directly\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Initialise global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "### Store stats:\n",
    "# Creates a node in the graph \n",
    "# that will evaluate the MSE value and write it\n",
    "# to a TensorBoard-compatible binary log string called a 'summary'\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "# Creates a FileWriter that you will use to write \n",
    "# summaries to logfiles in the log directory\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Function to fetch the mini-batch data\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)    # not shown in the book\n",
    "    indices = np.random.randint(m, size = batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]   # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]   # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "# Run the graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialise\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            ### Store stats about the MSE every 10 mini-batches\n",
    "            if batch_index % 10 == 0:\n",
    "                # Fetch the summary\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            # Run for next\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    # Get best theta\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "# Close the writer\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
